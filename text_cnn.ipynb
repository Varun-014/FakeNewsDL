{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from keras.layers import MaxPool1D, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Body'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>four ways bob corker skewered donald trump</td>\n",
       "      <td>image copyright getty images\\non sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>london (reuters) - “last flag flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>trump’s fight with corker jeopardizes his legi...</td>\n",
       "      <td>the feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>egypt's cheiron wins tie-up with pemex for mex...</td>\n",
       "      <td>mexico city (reuters) - egypt’s cheiron holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>jason aldean opens 'snl' with vegas tribute</td>\n",
       "      <td>country singer jason aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         four ways bob corker skewered donald trump   \n",
       "1  linklater's war veteran comedy speaks to moder...   \n",
       "2  trump’s fight with corker jeopardizes his legi...   \n",
       "3  egypt's cheiron wins tie-up with pemex for mex...   \n",
       "4        jason aldean opens 'snl' with vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  image copyright getty images\\non sunday mornin...      1  \n",
       "1  london (reuters) - “last flag flying”, a comed...      1  \n",
       "2  the feud broke into public view last week when...      1  \n",
       "3  mexico city (reuters) - egypt’s cheiron holdin...      1  \n",
       "4  country singer jason aldean, who was performin...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Headline'] = df['Headline'].str.lower()\n",
    "df['Body'] = df['Body'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuations(text):\n",
    "    punctuations = set(string.punctuation)\n",
    "    text = str(text)\n",
    "    # return text.translate(str.maketrans('', '', punctuations))\n",
    "    return \" \".join([word for word in text.split() if word not in punctuations])\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_punctuations(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_stopwords(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_spl_chars(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_spl_chars(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_spl_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def stem(text):\n",
    "    stemmed_sentence = \" \".join(stemmer.stem(word) for word in text.split())\n",
    "    return stemmed_sentence\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: stem(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_url(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3988"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "for i in range(3988):\n",
    "    texts.append(df['Body'][i])\n",
    "    labels.append(df['Label'][i])\n",
    "\n",
    "print(len(texts))\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_seq_length = 1000\n",
    "# the percentage of train test split to be applied\n",
    "validation_split = 0.2\n",
    "# the dimension of vectors to be used\n",
    "embedding_dim = 100\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 30\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32035"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': 1,\n",
       " 'said': 2,\n",
       " 'one': 3,\n",
       " 'year': 4,\n",
       " 'time': 5,\n",
       " 't': 6,\n",
       " 'new': 7,\n",
       " 'would': 8,\n",
       " 'game': 9,\n",
       " 'like': 10,\n",
       " 'state': 11,\n",
       " 'it': 12,\n",
       " 'also': 13,\n",
       " 'two': 14,\n",
       " 'week': 15,\n",
       " 'first': 16,\n",
       " 'trump': 17,\n",
       " 'say': 18,\n",
       " 'go': 19,\n",
       " 'peopl': 20,\n",
       " 'i': 21,\n",
       " 'get': 22,\n",
       " 'play': 23,\n",
       " '1': 24,\n",
       " 'us': 25,\n",
       " 'report': 26,\n",
       " 'make': 27,\n",
       " 'news': 28,\n",
       " 'last': 29,\n",
       " 'use': 30,\n",
       " 'mr': 31,\n",
       " 'presid': 32,\n",
       " 'back': 33,\n",
       " 'world': 34,\n",
       " 'could': 35,\n",
       " '2017': 36,\n",
       " 'team': 37,\n",
       " 'nation': 38,\n",
       " 'take': 39,\n",
       " 'com': 40,\n",
       " '2': 41,\n",
       " 'work': 42,\n",
       " 'photo': 43,\n",
       " 'govern': 44,\n",
       " 'day': 45,\n",
       " 'run': 46,\n",
       " 'stori': 47,\n",
       " 'u': 48,\n",
       " 'season': 49,\n",
       " 'come': 50,\n",
       " 'look': 51,\n",
       " 'may': 52,\n",
       " 'includ': 53,\n",
       " 'we': 54,\n",
       " '3': 55,\n",
       " 'right': 56,\n",
       " 'even': 57,\n",
       " 'that': 58,\n",
       " 'mani': 59,\n",
       " 'call': 60,\n",
       " 'imag': 61,\n",
       " 'countri': 62,\n",
       " 'unit': 63,\n",
       " 'content': 64,\n",
       " '5': 65,\n",
       " 'start': 66,\n",
       " '4': 67,\n",
       " 'the': 68,\n",
       " 'sourc': 69,\n",
       " 'compani': 70,\n",
       " 'he': 71,\n",
       " 'continu': 72,\n",
       " 'player': 73,\n",
       " 'reuter': 74,\n",
       " '10': 75,\n",
       " 'show': 76,\n",
       " 'three': 77,\n",
       " 'way': 78,\n",
       " 'need': 79,\n",
       " 'want': 80,\n",
       " 'think': 81,\n",
       " 'home': 82,\n",
       " 'told': 83,\n",
       " 'ad': 84,\n",
       " 'see': 85,\n",
       " 'point': 86,\n",
       " 'end': 87,\n",
       " 'american': 88,\n",
       " 'still': 89,\n",
       " 'made': 90,\n",
       " 'help': 91,\n",
       " 'sinc': 92,\n",
       " 'power': 93,\n",
       " 're': 94,\n",
       " 'york': 95,\n",
       " 'know': 96,\n",
       " 'good': 97,\n",
       " 'law': 98,\n",
       " 'yard': 99,\n",
       " 'polic': 100,\n",
       " 'group': 101,\n",
       " 'http': 102,\n",
       " 'second': 103,\n",
       " 'follow': 104,\n",
       " 'well': 105,\n",
       " 'receiv': 106,\n",
       " 'much': 107,\n",
       " 'tri': 108,\n",
       " 'part': 109,\n",
       " 'read': 110,\n",
       " 'long': 111,\n",
       " 'win': 112,\n",
       " 'month': 113,\n",
       " 'thing': 114,\n",
       " 'caption': 115,\n",
       " 'support': 116,\n",
       " 'defens': 117,\n",
       " 'octob': 118,\n",
       " 'appear': 119,\n",
       " 'pass': 120,\n",
       " 'www': 121,\n",
       " 'offici': 122,\n",
       " 'anoth': 123,\n",
       " 'hous': 124,\n",
       " 'befor': 125,\n",
       " 'live': 126,\n",
       " 'media': 127,\n",
       " 'plan': 128,\n",
       " 'open': 129,\n",
       " 'famili': 130,\n",
       " 'fire': 131,\n",
       " 'video': 132,\n",
       " 'north': 133,\n",
       " 'million': 134,\n",
       " 'allow': 135,\n",
       " 'place': 136,\n",
       " 'secur': 137,\n",
       " 'there': 138,\n",
       " 'sign': 139,\n",
       " 'forc': 140,\n",
       " 'accord': 141,\n",
       " '000': 142,\n",
       " 'around': 143,\n",
       " '9': 144,\n",
       " '6': 145,\n",
       " 'next': 146,\n",
       " 'post': 147,\n",
       " 'issu': 148,\n",
       " 'citi': 149,\n",
       " 'must': 150,\n",
       " 'line': 151,\n",
       " 'public': 152,\n",
       " 'pleas': 153,\n",
       " 'give': 154,\n",
       " 'night': 155,\n",
       " 'provid': 156,\n",
       " 'high': 157,\n",
       " 'chang': 158,\n",
       " 'happen': 159,\n",
       " 'twitter': 160,\n",
       " 'polit': 161,\n",
       " 'offic': 162,\n",
       " 'weinstein': 163,\n",
       " 'don': 164,\n",
       " 'top': 165,\n",
       " 'main': 166,\n",
       " 'deal': 167,\n",
       " 'base': 168,\n",
       " 'sport': 169,\n",
       " 'move': 170,\n",
       " 'm': 171,\n",
       " 'raven': 172,\n",
       " 'women': 173,\n",
       " 'four': 174,\n",
       " 'you': 175,\n",
       " 'hit': 176,\n",
       " 'fact': 177,\n",
       " '7': 178,\n",
       " 'system': 179,\n",
       " 'a': 180,\n",
       " 'set': 181,\n",
       " 'final': 182,\n",
       " 'former': 183,\n",
       " 'leagu': 184,\n",
       " 'white': 185,\n",
       " 'left': 186,\n",
       " 'sunday': 187,\n",
       " 'found': 188,\n",
       " 'septemb': 189,\n",
       " 'author': 190,\n",
       " 'find': 191,\n",
       " 'attack': 192,\n",
       " 'person': 193,\n",
       " 'ask': 194,\n",
       " 'room': 195,\n",
       " 'best': 196,\n",
       " 'howev': 197,\n",
       " 'servic': 198,\n",
       " 'put': 199,\n",
       " 'relat': 200,\n",
       " 'question': 201,\n",
       " 'investig': 202,\n",
       " 'number': 203,\n",
       " 've': 204,\n",
       " 'big': 205,\n",
       " 'talk': 206,\n",
       " 'man': 207,\n",
       " 'parti': 208,\n",
       " 'never': 209,\n",
       " 'recent': 210,\n",
       " 'intern': 211,\n",
       " 'nfl': 212,\n",
       " 'articl': 213,\n",
       " 'product': 214,\n",
       " 'realli': 215,\n",
       " '8': 216,\n",
       " 'california': 217,\n",
       " 'believ': 218,\n",
       " 'record': 219,\n",
       " 'war': 220,\n",
       " 'field': 221,\n",
       " 'old': 222,\n",
       " 'major': 223,\n",
       " 'later': 224,\n",
       " 'face': 225,\n",
       " 'statement': 226,\n",
       " 'lead': 227,\n",
       " 'jet': 228,\n",
       " 'offer': 229,\n",
       " 'event': 230,\n",
       " 'differ': 231,\n",
       " 'case': 232,\n",
       " 'percent': 233,\n",
       " 'expect': 234,\n",
       " 'fan': 235,\n",
       " 'up': 236,\n",
       " 'militari': 237,\n",
       " 'great': 238,\n",
       " 'washington': 239,\n",
       " 'univers': 240,\n",
       " 'shot': 241,\n",
       " 'to': 242,\n",
       " 'america': 243,\n",
       " 'facebook': 244,\n",
       " 'lot': 245,\n",
       " 'member': 246,\n",
       " 'someth': 247,\n",
       " 'everi': 248,\n",
       " 'shoot': 249,\n",
       " 'past': 250,\n",
       " 'without': 251,\n",
       " 'market': 252,\n",
       " 'data': 253,\n",
       " 'free': 254,\n",
       " 'paddock': 255,\n",
       " '30': 256,\n",
       " 'order': 257,\n",
       " 'control': 258,\n",
       " 'vega': 259,\n",
       " 'friend': 260,\n",
       " 'and': 261,\n",
       " 'offens': 262,\n",
       " 'add': 263,\n",
       " 'five': 264,\n",
       " 'car': 265,\n",
       " 'they': 266,\n",
       " 'health': 267,\n",
       " 'hope': 268,\n",
       " 'busi': 269,\n",
       " 'came': 270,\n",
       " 'head': 271,\n",
       " 'watch': 272,\n",
       " '2016': 273,\n",
       " 'becom': 274,\n",
       " 'might': 275,\n",
       " 'hide': 276,\n",
       " '0': 277,\n",
       " 'tuesday': 278,\n",
       " 'inform': 279,\n",
       " 'got': 280,\n",
       " 'campaign': 281,\n",
       " 'name': 282,\n",
       " 'remain': 283,\n",
       " 'court': 284,\n",
       " 'oper': 285,\n",
       " 'better': 286,\n",
       " 'perform': 287,\n",
       " 'newslett': 288,\n",
       " 'turn': 289,\n",
       " 'manag': 290,\n",
       " 'view': 291,\n",
       " 'wednesday': 292,\n",
       " 'feder': 293,\n",
       " 'goal': 294,\n",
       " 'russia': 295,\n",
       " 'out': 296,\n",
       " 'act': 297,\n",
       " 'polici': 298,\n",
       " 'near': 299,\n",
       " 'seem': 300,\n",
       " 'vs': 301,\n",
       " 'tax': 302,\n",
       " 'sever': 303,\n",
       " 'research': 304,\n",
       " 'mean': 305,\n",
       " 'life': 306,\n",
       " 'list': 307,\n",
       " 'hand': 308,\n",
       " 'elect': 309,\n",
       " 'music': 310,\n",
       " '11': 311,\n",
       " 'keep': 312,\n",
       " 'china': 313,\n",
       " 'footbal': 314,\n",
       " 'far': 315,\n",
       " 'monday': 316,\n",
       " 'cup': 317,\n",
       " 'here': 318,\n",
       " 'korea': 319,\n",
       " 'took': 320,\n",
       " 'this': 321,\n",
       " 'interest': 322,\n",
       " 'rule': 323,\n",
       " 'went': 324,\n",
       " 'score': 325,\n",
       " 'advertis': 326,\n",
       " 'least': 327,\n",
       " 'possibl': 328,\n",
       " 'clear': 329,\n",
       " 'third': 330,\n",
       " 'carri': 331,\n",
       " 'independ': 332,\n",
       " 'in': 333,\n",
       " 'kill': 334,\n",
       " 'legal': 335,\n",
       " 'link': 336,\n",
       " 'creat': 337,\n",
       " 'las': 338,\n",
       " 'stand': 339,\n",
       " 'among': 340,\n",
       " 'south': 341,\n",
       " 'cnn': 342,\n",
       " 'away': 343,\n",
       " 'releas': 344,\n",
       " 'let': 345,\n",
       " 'love': 346,\n",
       " 'decis': 347,\n",
       " 'now': 348,\n",
       " 'earli': 349,\n",
       " 'side': 350,\n",
       " 'meet': 351,\n",
       " 'leader': 352,\n",
       " 'close': 353,\n",
       " 'd': 354,\n",
       " 'alleg': 355,\n",
       " 'alway': 356,\n",
       " 'feel': 357,\n",
       " 'rate': 358,\n",
       " 'miss': 359,\n",
       " 'alreadi': 360,\n",
       " 'share': 361,\n",
       " 'respons': 362,\n",
       " 'job': 363,\n",
       " '20': 364,\n",
       " 'option': 365,\n",
       " 'them': 366,\n",
       " 'whether': 367,\n",
       " 'import': 368,\n",
       " 'can': 369,\n",
       " 'target': 370,\n",
       " 'consid': 371,\n",
       " '12': 372,\n",
       " 'learn': 373,\n",
       " 'develop': 374,\n",
       " 'yet': 375,\n",
       " 'claim': 376,\n",
       " 'administr': 377,\n",
       " 'though': 378,\n",
       " 'hard': 379,\n",
       " 'seen': 380,\n",
       " 'protect': 381,\n",
       " 'gun': 382,\n",
       " 'democrat': 383,\n",
       " 'histori': 384,\n",
       " 'current': 385,\n",
       " 'posit': 386,\n",
       " 'flag': 387,\n",
       " 'comment': 388,\n",
       " 'action': 389,\n",
       " 'littl': 390,\n",
       " 'result': 391,\n",
       " 'click': 392,\n",
       " 'tell': 393,\n",
       " 'leav': 394,\n",
       " 'reader': 395,\n",
       " 'file': 396,\n",
       " 'school': 397,\n",
       " 'special': 398,\n",
       " 'vote': 399,\n",
       " '15': 400,\n",
       " 'ms': 401,\n",
       " 'donald': 402,\n",
       " 'human': 403,\n",
       " 'thought': 404,\n",
       " 'money': 405,\n",
       " 'protest': 406,\n",
       " 'iran': 407,\n",
       " 'bill': 408,\n",
       " 'book': 409,\n",
       " 'visit': 410,\n",
       " 'stop': 411,\n",
       " 'term': 412,\n",
       " 'area': 413,\n",
       " 'known': 414,\n",
       " 'real': 415,\n",
       " 'problem': 416,\n",
       " 'return': 417,\n",
       " 'full': 418,\n",
       " 'bay': 419,\n",
       " 'potenti': 420,\n",
       " 'agenc': 421,\n",
       " 'warn': 422,\n",
       " 'him': 423,\n",
       " 'enter': 424,\n",
       " 'pick': 425,\n",
       " 'program': 426,\n",
       " 'https': 427,\n",
       " '17': 428,\n",
       " 'other': 429,\n",
       " 'reason': 430,\n",
       " 'anthem': 431,\n",
       " 'abl': 432,\n",
       " 'organ': 433,\n",
       " 'film': 434,\n",
       " 'light': 435,\n",
       " 'red': 436,\n",
       " 'll': 437,\n",
       " 'depart': 438,\n",
       " 'technolog': 439,\n",
       " 'site': 440,\n",
       " 'rank': 441,\n",
       " 'on': 442,\n",
       " 'center': 443,\n",
       " 'given': 444,\n",
       " 'local': 445,\n",
       " 'note': 446,\n",
       " 'build': 447,\n",
       " 'today': 448,\n",
       " 'discuss': 449,\n",
       " 'caus': 450,\n",
       " 'industri': 451,\n",
       " 'led': 452,\n",
       " 'weapon': 453,\n",
       " 'subscrib': 454,\n",
       " 'critic': 455,\n",
       " 'senat': 456,\n",
       " 'steeler': 457,\n",
       " 'global': 458,\n",
       " 'devic': 459,\n",
       " 'drive': 460,\n",
       " 'bear': 461,\n",
       " 'produc': 462,\n",
       " 'address': 463,\n",
       " 'minut': 464,\n",
       " 'foreign': 465,\n",
       " 'publish': 466,\n",
       " 'co': 467,\n",
       " 'reach': 468,\n",
       " 'minist': 469,\n",
       " 'effect': 470,\n",
       " 'general': 471,\n",
       " 'involv': 472,\n",
       " 'behind': 473,\n",
       " 'increas': 474,\n",
       " 'effort': 475,\n",
       " 'origin': 476,\n",
       " 'daili': 477,\n",
       " 'ball': 478,\n",
       " 'trade': 479,\n",
       " 'coach': 480,\n",
       " 'all': 481,\n",
       " 'young': 482,\n",
       " 'russian': 483,\n",
       " 'conservativedailynew': 484,\n",
       " 'republican': 485,\n",
       " 'ever': 486,\n",
       " 'agre': 487,\n",
       " 'loss': 488,\n",
       " 'email': 489,\n",
       " 'children': 490,\n",
       " 'level': 491,\n",
       " 'thank': 492,\n",
       " 'across': 493,\n",
       " 'announc': 494,\n",
       " 'speak': 495,\n",
       " 'concern': 496,\n",
       " 'hold': 497,\n",
       " 'european': 498,\n",
       " 'region': 499,\n",
       " 'copyright': 500,\n",
       " 'pay': 501,\n",
       " '100': 502,\n",
       " 'complet': 503,\n",
       " 'social': 504,\n",
       " 'six': 505,\n",
       " 'chief': 506,\n",
       " 'done': 507,\n",
       " 'seri': 508,\n",
       " 'often': 509,\n",
       " 'project': 510,\n",
       " 'thursday': 511,\n",
       " 'billion': 512,\n",
       " 'chanc': 513,\n",
       " 'check': 514,\n",
       " 'injuri': 515,\n",
       " '23': 516,\n",
       " 'star': 517,\n",
       " 'limit': 518,\n",
       " 'die': 519,\n",
       " 'interview': 520,\n",
       " 'nuclear': 521,\n",
       " 'direct': 522,\n",
       " '25': 523,\n",
       " 'vaccin': 524,\n",
       " 'natur': 525,\n",
       " 'account': 526,\n",
       " 'lost': 527,\n",
       " 'process': 528,\n",
       " 'bad': 529,\n",
       " 'sure': 530,\n",
       " 'bank': 531,\n",
       " 'step': 532,\n",
       " 'total': 533,\n",
       " 'sell': 534,\n",
       " 'bodi': 535,\n",
       " 'studi': 536,\n",
       " '2015': 537,\n",
       " 'enough': 538,\n",
       " 'success': 539,\n",
       " 'half': 540,\n",
       " 'actual': 541,\n",
       " 'conserv': 542,\n",
       " 'press': 543,\n",
       " 'fight': 544,\n",
       " 'futur': 545,\n",
       " 'ban': 546,\n",
       " 'hour': 547,\n",
       " '18': 548,\n",
       " 'outsid': 549,\n",
       " 'along': 550,\n",
       " 'charg': 551,\n",
       " '24': 552,\n",
       " 'network': 553,\n",
       " 'togeth': 554,\n",
       " 'less': 555,\n",
       " 'singl': 556,\n",
       " 'off': 557,\n",
       " 'mexico': 558,\n",
       " 'late': 559,\n",
       " 'board': 560,\n",
       " 'execut': 561,\n",
       " 'wide': 562,\n",
       " 'amaz': 563,\n",
       " 'challeng': 564,\n",
       " 'short': 565,\n",
       " 'pressur': 566,\n",
       " 'defend': 567,\n",
       " 'role': 568,\n",
       " 'matter': 569,\n",
       " '2018': 570,\n",
       " 'hurrican': 571,\n",
       " 'smart': 572,\n",
       " 'avail': 573,\n",
       " 'accus': 574,\n",
       " 'risk': 575,\n",
       " 'arm': 576,\n",
       " 'director': 577,\n",
       " 'strong': 578,\n",
       " 'committe': 579,\n",
       " 'eye': 580,\n",
       " 'air': 581,\n",
       " 'due': 582,\n",
       " 'select': 583,\n",
       " 'men': 584,\n",
       " 'green': 585,\n",
       " 'water': 586,\n",
       " 'per': 587,\n",
       " 'certain': 588,\n",
       " 'form': 589,\n",
       " 'wildfir': 590,\n",
       " 'j': 591,\n",
       " 'do': 592,\n",
       " 'chicago': 593,\n",
       " 'cent': 594,\n",
       " 'fund': 595,\n",
       " 'death': 596,\n",
       " 'john': 597,\n",
       " 'experi': 598,\n",
       " '16': 599,\n",
       " 'black': 600,\n",
       " 'individu': 601,\n",
       " '13': 602,\n",
       " 'cut': 603,\n",
       " 'break': 604,\n",
       " 'taken': 605,\n",
       " 'test': 606,\n",
       " 'activ': 607,\n",
       " 'fail': 608,\n",
       " 'what': 609,\n",
       " 'attempt': 610,\n",
       " 'rais': 611,\n",
       " 'communiti': 612,\n",
       " 'so': 613,\n",
       " 'hotel': 614,\n",
       " 'stay': 615,\n",
       " 'travel': 616,\n",
       " 'england': 617,\n",
       " 'didn': 618,\n",
       " '14': 619,\n",
       " 'career': 620,\n",
       " '2014': 621,\n",
       " 'previous': 622,\n",
       " 'qualifi': 623,\n",
       " 'pictur': 624,\n",
       " 'august': 625,\n",
       " 'toward': 626,\n",
       " 'despit': 627,\n",
       " 'opportun': 628,\n",
       " 'remov': 629,\n",
       " 'bring': 630,\n",
       " 'forward': 631,\n",
       " 'focus': 632,\n",
       " 'san': 633,\n",
       " 'serv': 634,\n",
       " 'anyth': 635,\n",
       " 'price': 636,\n",
       " 'noth': 637,\n",
       " 'match': 638,\n",
       " 'requir': 639,\n",
       " 'wrote': 640,\n",
       " 'struggl': 641,\n",
       " 'oppos': 642,\n",
       " 'cours': 643,\n",
       " 'within': 644,\n",
       " '40': 645,\n",
       " 'larg': 646,\n",
       " 'instead': 647,\n",
       " 'associ': 648,\n",
       " 'friday': 649,\n",
       " '29': 650,\n",
       " 'detail': 651,\n",
       " 'central': 652,\n",
       " 'care': 653,\n",
       " 'secretari': 654,\n",
       " 'decid': 655,\n",
       " 'begin': 656,\n",
       " 'practic': 657,\n",
       " 'n': 658,\n",
       " 'occur': 659,\n",
       " 'ago': 660,\n",
       " 'baltimor': 661,\n",
       " 'street': 662,\n",
       " 'quarterback': 663,\n",
       " 'repres': 664,\n",
       " 'owner': 665,\n",
       " 'suggest': 666,\n",
       " 'join': 667,\n",
       " 'brown': 668,\n",
       " 'is': 669,\n",
       " 'held': 670,\n",
       " 'lose': 671,\n",
       " 'congress': 672,\n",
       " 'safeti': 673,\n",
       " '50': 674,\n",
       " 'train': 675,\n",
       " 'japan': 676,\n",
       " 'round': 677,\n",
       " 'bbc': 678,\n",
       " 'tweet': 679,\n",
       " 'decad': 680,\n",
       " 'food': 681,\n",
       " 'puerto': 682,\n",
       " 'everyon': 683,\n",
       " 'beat': 684,\n",
       " 'me': 685,\n",
       " 'host': 686,\n",
       " 'staff': 687,\n",
       " 'harvey': 688,\n",
       " 'websit': 689,\n",
       " 'celebr': 690,\n",
       " 'mark': 691,\n",
       " 'angel': 692,\n",
       " 'walk': 693,\n",
       " 'key': 694,\n",
       " 'averag': 695,\n",
       " 'quick': 696,\n",
       " 'korean': 697,\n",
       " 'econom': 698,\n",
       " 'financi': 699,\n",
       " 'no': 700,\n",
       " 'sexual': 701,\n",
       " 'almost': 702,\n",
       " 'collect': 703,\n",
       " 'joe': 704,\n",
       " 'evid': 705,\n",
       " 'contact': 706,\n",
       " 'situat': 707,\n",
       " 'access': 708,\n",
       " 'eight': 709,\n",
       " 'type': 710,\n",
       " 'insid': 711,\n",
       " 'confer': 712,\n",
       " 'seek': 713,\n",
       " 'box': 714,\n",
       " 'economi': 715,\n",
       " 'yanke': 716,\n",
       " 'saturday': 717,\n",
       " 'giant': 718,\n",
       " 'updat': 719,\n",
       " 'guy': 720,\n",
       " '09': 721,\n",
       " 'document': 722,\n",
       " 'intellig': 723,\n",
       " 'firm': 724,\n",
       " 'down': 725,\n",
       " 'border': 726,\n",
       " 'emerg': 727,\n",
       " 'addit': 728,\n",
       " 'td': 729,\n",
       " 'moment': 730,\n",
       " 'especi': 731,\n",
       " 'obama': 732,\n",
       " 'probabl': 733,\n",
       " 'race': 734,\n",
       " 'push': 735,\n",
       " 'kind': 736,\n",
       " 'cover': 737,\n",
       " '19': 738,\n",
       " 'pittsburgh': 739,\n",
       " 'throw': 740,\n",
       " 'girl': 741,\n",
       " 'everyth': 742,\n",
       " 'mass': 743,\n",
       " 'understand': 744,\n",
       " 'saw': 745,\n",
       " 'deni': 746,\n",
       " 'tv': 747,\n",
       " 'seven': 748,\n",
       " 'word': 749,\n",
       " 'cdn': 750,\n",
       " 'privaci': 751,\n",
       " 'woman': 752,\n",
       " 'london': 753,\n",
       " 'c': 754,\n",
       " 'doesn': 755,\n",
       " 'gave': 756,\n",
       " 'write': 757,\n",
       " 'age': 758,\n",
       " 'los': 759,\n",
       " 'pitch': 760,\n",
       " 'again': 761,\n",
       " 'cost': 762,\n",
       " 'earlier': 763,\n",
       " 'began': 764,\n",
       " 'small': 765,\n",
       " 'institut': 766,\n",
       " 'util': 767,\n",
       " 'commiss': 768,\n",
       " 'credit': 769,\n",
       " 'display': 770,\n",
       " 'review': 771,\n",
       " 'answer': 772,\n",
       " 'declar': 773,\n",
       " 'similar': 774,\n",
       " 'road': 775,\n",
       " 'googl': 776,\n",
       " 'buy': 777,\n",
       " 'wife': 778,\n",
       " 'damag': 779,\n",
       " 'meter': 780,\n",
       " 'front': 781,\n",
       " 'victim': 782,\n",
       " 'biggest': 783,\n",
       " 'privat': 784,\n",
       " 'serious': 785,\n",
       " 'immigr': 786,\n",
       " 'design': 787,\n",
       " 'mind': 788,\n",
       " 'p': 789,\n",
       " 'scienc': 790,\n",
       " 'whose': 791,\n",
       " 'simpl': 792,\n",
       " 'morn': 793,\n",
       " 'coupl': 794,\n",
       " 'victori': 795,\n",
       " 'oil': 796,\n",
       " 'pro': 797,\n",
       " 'finish': 798,\n",
       " 'trick': 799,\n",
       " 'ground': 800,\n",
       " 'stage': 801,\n",
       " 'corpor': 802,\n",
       " '26': 803,\n",
       " 'prepar': 804,\n",
       " 'song': 805,\n",
       " 'respond': 806,\n",
       " '35': 807,\n",
       " 'rico': 808,\n",
       " 'west': 809,\n",
       " 'drug': 810,\n",
       " 'michael': 811,\n",
       " 'inning': 812,\n",
       " 'consum': 813,\n",
       " 'rout': 814,\n",
       " 'britain': 815,\n",
       " 'drop': 816,\n",
       " 'describ': 817,\n",
       " 'grow': 818,\n",
       " 'arrest': 819,\n",
       " 'freedom': 820,\n",
       " 'measur': 821,\n",
       " 'bit': 822,\n",
       " 'land': 823,\n",
       " 'heart': 824,\n",
       " 'island': 825,\n",
       " '32': 826,\n",
       " 'own': 827,\n",
       " 'period': 828,\n",
       " 'enforc': 829,\n",
       " 'award': 830,\n",
       " 'entir': 831,\n",
       " 'agreement': 832,\n",
       " '500': 833,\n",
       " 'messag': 834,\n",
       " 'initi': 835,\n",
       " 'touchdown': 836,\n",
       " 'latest': 837,\n",
       " 'reveal': 838,\n",
       " 'valu': 839,\n",
       " 'sale': 840,\n",
       " 'syria': 841,\n",
       " 'art': 842,\n",
       " 'onlin': 843,\n",
       " 'particular': 844,\n",
       " 'basebal': 845,\n",
       " 'explain': 846,\n",
       " 'common': 847,\n",
       " 'constitut': 848,\n",
       " 'fantasi': 849,\n",
       " 'medic': 850,\n",
       " 'brazil': 851,\n",
       " 'playoff': 852,\n",
       " 'british': 853,\n",
       " 'rest': 854,\n",
       " 'figur': 855,\n",
       " 'union': 856,\n",
       " 'parent': 857,\n",
       " '28': 858,\n",
       " 'energi': 859,\n",
       " 'germani': 860,\n",
       " 'propos': 861,\n",
       " 'dollar': 862,\n",
       " 'europ': 863,\n",
       " 'artist': 864,\n",
       " 'flacco': 865,\n",
       " 'immedi': 866,\n",
       " 'juli': 867,\n",
       " 'save': 868,\n",
       " 'simpli': 869,\n",
       " 'improv': 870,\n",
       " 'sanction': 871,\n",
       " 'space': 872,\n",
       " 'either': 873,\n",
       " '22': 874,\n",
       " 'al': 875,\n",
       " 'catch': 876,\n",
       " 'uk': 877,\n",
       " 'sit': 878,\n",
       " 'thousand': 879,\n",
       " 'commit': 880,\n",
       " '21': 881,\n",
       " 'r': 882,\n",
       " 'idea': 883,\n",
       " 'resid': 884,\n",
       " 'turkey': 885,\n",
       " 'titl': 886,\n",
       " 'anti': 887,\n",
       " 'demand': 888,\n",
       " 'but': 889,\n",
       " 'earth': 890,\n",
       " 'confirm': 891,\n",
       " 'low': 892,\n",
       " 'respect': 893,\n",
       " 'middl': 894,\n",
       " 'prime': 895,\n",
       " 'fall': 896,\n",
       " 'custom': 897,\n",
       " 'if': 898,\n",
       " 'mike': 899,\n",
       " 'search': 900,\n",
       " 'shock': 901,\n",
       " 'non': 902,\n",
       " 'guard': 903,\n",
       " 'affect': 904,\n",
       " 'featur': 905,\n",
       " 'more': 906,\n",
       " 'o': 907,\n",
       " 'judg': 908,\n",
       " 'for': 909,\n",
       " 'ahead': 910,\n",
       " 'ensur': 911,\n",
       " 'benefit': 912,\n",
       " 'wall': 913,\n",
       " 'texa': 914,\n",
       " 'dead': 915,\n",
       " 'track': 916,\n",
       " 'isn': 917,\n",
       " 'present': 918,\n",
       " 'amazon': 919,\n",
       " 'lack': 920,\n",
       " 'indic': 921,\n",
       " 'dr': 922,\n",
       " 'illeg': 923,\n",
       " 'surpris': 924,\n",
       " 'of': 925,\n",
       " 'gain': 926,\n",
       " 'condit': 927,\n",
       " 'fourth': 928,\n",
       " 'club': 929,\n",
       " 'connect': 930,\n",
       " 'although': 931,\n",
       " 'door': 932,\n",
       " 'upon': 933,\n",
       " 'legisl': 934,\n",
       " 'phone': 935,\n",
       " 'accept': 936,\n",
       " 'block': 937,\n",
       " 'worker': 938,\n",
       " 'exist': 939,\n",
       " 'easi': 940,\n",
       " 'counti': 941,\n",
       " 'error': 942,\n",
       " 'refer': 943,\n",
       " 'tie': 944,\n",
       " '27': 945,\n",
       " 'impact': 946,\n",
       " 'generat': 947,\n",
       " 'crime': 948,\n",
       " 'paid': 949,\n",
       " 'burn': 950,\n",
       " 'candid': 951,\n",
       " 'not': 952,\n",
       " 'someon': 953,\n",
       " 'signific': 954,\n",
       " 'gone': 955,\n",
       " 'threat': 956,\n",
       " 'june': 957,\n",
       " 'smith': 958,\n",
       " 'india': 959,\n",
       " 'identifi': 960,\n",
       " 'deep': 961,\n",
       " 'danger': 962,\n",
       " 'regular': 963,\n",
       " 'via': 964,\n",
       " 'park': 965,\n",
       " 'advanc': 966,\n",
       " 'hear': 967,\n",
       " 'cultur': 968,\n",
       " 'violat': 969,\n",
       " 'stephen': 970,\n",
       " 'deliv': 971,\n",
       " 'stock': 972,\n",
       " 'licens': 973,\n",
       " 'opposit': 974,\n",
       " 'spot': 975,\n",
       " 'david': 976,\n",
       " 'rush': 977,\n",
       " 'franc': 978,\n",
       " 'corker': 979,\n",
       " 'fear': 980,\n",
       " 'festiv': 981,\n",
       " 'sent': 982,\n",
       " 'march': 983,\n",
       " 'regul': 984,\n",
       " 'eu': 985,\n",
       " 'quit': 986,\n",
       " 'strike': 987,\n",
       " 'spain': 988,\n",
       " 'weekend': 989,\n",
       " 'who': 990,\n",
       " 'student': 991,\n",
       " 'regard': 992,\n",
       " 'tech': 993,\n",
       " 'profession': 994,\n",
       " 'draft': 995,\n",
       " 'violenc': 996,\n",
       " 'coverag': 997,\n",
       " 'arriv': 998,\n",
       " 'rise': 999,\n",
       " 'shooter': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"this is a car\"\n",
    "seq = tokenizer.texts_to_sequences([a])\n",
    "d = pad_sequences(seq, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0, 321, 669, 180, 265]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {'Fake': 1, 'Not_Fake': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:  (3988, 1000)\n",
      "Shape of label tensor:  (3988, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data tensor: \", data.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3191, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(797, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_idx = {}\n",
    "f = open(\"./glove.6B.100d.txt\", encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_idx[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32035"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.13739   ,  0.77890998,  0.80053997, ..., -0.61676002,\n",
       "         0.44703001, -0.27967   ],\n",
       "       [-0.13128   , -0.45199999,  0.043399  , ..., -0.30526   ,\n",
       "        -0.045495  ,  0.56509   ],\n",
       "       ...,\n",
       "       [-0.12560999, -0.020344  , -0.31240001, ...,  0.38492   ,\n",
       "        -0.55392998, -0.29003999],\n",
       "       [-0.017809  ,  0.74079001,  0.37839001, ...,  0.23705   ,\n",
       "        -0.41218999, -0.090674  ],\n",
       "       [ 0.17927   ,  0.23128   , -0.55498999, ...,  0.43808001,\n",
       "         0.15075   , -0.59816998]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index)+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_seq_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1000, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "embedding = embedding_layer(inputs)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1000, 100, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_inp = Reshape((max_seq_length, embedding_dim, 1))(embedding)\n",
    "reshaped_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 1, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding=\"valid\", kernel_initializer='normal',\n",
    "                activation='relu')(reshaped_inp)\n",
    "maxpool_1 = MaxPool2D(pool_size=(max_seq_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 1, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding=\"valid\", kernel_initializer='normal',\n",
    "                activation='relu')(reshaped_inp)\n",
    "maxpool_2 = MaxPool2D(pool_size=(max_seq_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "maxpool_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 1, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding=\"valid\", kernel_initializer='normal',\n",
    "                activation='relu')(reshaped_inp)\n",
    "maxpool_3 = MaxPool2D(pool_size=(max_seq_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_3)\n",
    "maxpool_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='sigmoid')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1000)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1000, 100)            3203600   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1000, 100, 1)         0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 998, 1, 512)          154112    ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 997, 1, 512)          205312    ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 996, 1, 512)          256512    ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 1, 1, 512)            0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 1, 512)            0         ['conv2d_1[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 512)            0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 3, 1, 512)            0         ['max_pooling2d[0][0]',       \n",
      "                                                                     'max_pooling2d_1[0][0]',     \n",
      "                                                                     'max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1536)                 0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1536)                 0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    3074      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3822610 (14.58 MB)\n",
      "Trainable params: 619010 (2.36 MB)\n",
      "Non-trainable params: 3203600 (12.22 MB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "adam = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Epoch 1/5\n",
      "107/107 [==============================] - 68s 630ms/step - loss: 0.3456 - accuracy: 0.8631 - val_loss: 0.1594 - val_accuracy: 0.9410\n",
      "Epoch 2/5\n",
      "107/107 [==============================] - 45s 421ms/step - loss: 0.2392 - accuracy: 0.9082 - val_loss: 0.1393 - val_accuracy: 0.9448\n",
      "Epoch 3/5\n",
      "107/107 [==============================] - 44s 412ms/step - loss: 0.1995 - accuracy: 0.9217 - val_loss: 0.1284 - val_accuracy: 0.9523\n",
      "Epoch 4/5\n",
      "107/107 [==============================] - 45s 419ms/step - loss: 0.1871 - accuracy: 0.9238 - val_loss: 0.1264 - val_accuracy: 0.9498\n",
      "Epoch 5/5\n",
      "107/107 [==============================] - 49s 462ms/step - loss: 0.1429 - accuracy: 0.9464 - val_loss: 0.1350 - val_accuracy: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24188e4ca30>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 1000)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_seq_length])\n",
    "    layer = Embedding(max_words,100,input_length=max_seq_length)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(2,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 1000, 100)         1000000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               16640     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 2)                 514       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1059394 (4.04 MB)\n",
      "Trainable params: 1059394 (4.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model_lstm = RNN()\n",
    "model_lstm.summary()\n",
    "model_lstm.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.6733 - accuracy: 0.6164 - val_loss: 0.5867 - val_accuracy: 0.8372\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 31s 2s/step - loss: 0.3717 - accuracy: 0.8856 - val_loss: 0.2292 - val_accuracy: 0.9264\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.1185 - accuracy: 0.9679 - val_loss: 0.1205 - val_accuracy: 0.9562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2418b07b7f0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_lstm.fit(x_train,y_train,batch_size=128,epochs=3,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  457, 1503,   36],\n",
       "       [   0,    0,    0, ...,  569, 2133, 3556],\n",
       "       [   0,    0,    0, ...,  187,    9,  408],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  768,  487,  888],\n",
       "       [   0,    0,    0, ...,  119,   87,  410],\n",
       "       [   0,    0,    0, ...,   51,   10,   56]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_lstm_json = model_lstm.to_json()\n",
    "with open(\"model_lstm_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_lstm_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_lstm.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 11s 111ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01096675, 0.9890069 ],\n",
       "       [0.00440007, 0.99562013],\n",
       "       [0.00657555, 0.9935213 ],\n",
       "       ...,\n",
       "       [0.9220456 , 0.07908832],\n",
       "       [0.9452604 , 0.05582776],\n",
       "       [0.9892207 , 0.01163793]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
