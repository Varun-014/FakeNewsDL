{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPAttention, self).__init__()\n",
    "\n",
    "        self.query_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        query_projection = self.query_projection(query)\n",
    "        key_projection = self.key_projection(keys)\n",
    "        value_projection = self.value_projection(values)\n",
    "\n",
    "        attention_weights = torch.matmul(query_projection, key_projection.transpose(-1, -2))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        attention_output = torch.matmul(attention_weights, value_projection)\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Bidirectional, TimeDistributed\n",
    "# from keras.layers import CuDNNGRU\n",
    "from keras.layers import GRU\n",
    "import keras\n",
    "\n",
    "class HCAN(Model):\n",
    "    def __init__(self,\n",
    "                 maxlen_sentence,\n",
    "                 maxlen_word,\n",
    "                 max_features,\n",
    "                 embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        super(HCAN, self).__init__()\n",
    "        self.maxlen_sentence = maxlen_sentence\n",
    "        self.maxlen_word = maxlen_word\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "        # Word part\n",
    "        input_word = Input(shape=(self.maxlen_word,))\n",
    "        x_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
    "        x_word = keras.layers.Convolution1D(100, 10, activation=\"relu\", padding = 'same')(x_word)\n",
    "        # x_word = layers.GlobalMaxPool1D()(x_word)\n",
    "\n",
    "        x_word = Bidirectional(GRU(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
    "        x_word = Attention(self.maxlen_word)(x_word)\n",
    "        model_word = Model(input_word, x_word)\n",
    "        # Sentence part\n",
    "        self.word_encoder_att = TimeDistributed(model_word)\n",
    "        self.sentence_encoder = Bidirectional(GRU(128, return_sequences=True))  # LSTM or GRU\n",
    "        self.sentence_att = Attention(self.maxlen_sentence)\n",
    "        # Output part\n",
    "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "            if len(inputs.get_shape()) != 3:\n",
    "                raise ValueError('The rank of inputs of HAN must be 3, but now is %d' % len(inputs.get_shape()))\n",
    "            if inputs.get_shape()[1] != self.maxlen_sentence:\n",
    "                raise ValueError('The maxlen_sentence of inputs of HAN must be %d, but now is %d' % (self.maxlen_sentence, inputs.get_shape()[1]))\n",
    "            if inputs.get_shape()[2] != self.maxlen_word:\n",
    "                raise ValueError('The maxlen_word of inputs of HAN must be %d, but now is %d' % (self.maxlen_word, inputs.get_shape()[2]))\n",
    "            x_sentence = self.word_encoder_att(inputs)\n",
    "            x_sentence = self.sentence_encoder(x_sentence)\n",
    "            x_sentence = self.sentence_att(x_sentence)\n",
    "            output = self.classifier(x_sentence)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>four ways bob corker skewered donald trump</td>\n",
       "      <td>image copyright getty images\\non sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>london (reuters) - âlast flag flyingâ, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>trumpâs fight with corker jeopardizes his legi...</td>\n",
       "      <td>the feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>egypt's cheiron wins tie-up with pemex for mex...</td>\n",
       "      <td>mexico city (reuters) - egyptâs cheiron holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>jason aldean opens 'snl' with vegas tribute</td>\n",
       "      <td>country singer jason aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         four ways bob corker skewered donald trump   \n",
       "1  linklater's war veteran comedy speaks to moder...   \n",
       "2  trumpâs fight with corker jeopardizes his legi...   \n",
       "3  egypt's cheiron wins tie-up with pemex for mex...   \n",
       "4        jason aldean opens 'snl' with vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  image copyright getty images\\non sunday mornin...      1  \n",
       "1  london (reuters) - âlast flag flyingâ, a comed...      1  \n",
       "2  the feud broke into public view last week when...      1  \n",
       "3  mexico city (reuters) - egyptâs cheiron holdin...      1  \n",
       "4  country singer jason aldean, who was performin...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "\n",
    "df.dropna(subset=['Body'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df['Headline'] = df['Headline'].str.lower()\n",
    "df['Body'] = df['Body'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Preprocessing\n",
    "import string\n",
    "def remove_punctuations(text):\n",
    "    punctuations = set(string.punctuation)\n",
    "    text = str(text)\n",
    "    # return text.translate(str.maketrans('', '', punctuations))\n",
    "    return \" \".join([word for word in text.split() if word not in punctuations])\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_punctuations(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_stopwords(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "import re\n",
    "def remove_spl_chars(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_spl_chars(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_spl_chars(x))\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def stem(text):\n",
    "    stemmed_sentence = \" \".join(stemmer.stem(word) for word in text.split())\n",
    "    return stemmed_sentence\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: stem(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: stem(x))\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(lambda x: remove_url(x))\n",
    "df['Body'] = df['Body'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3988"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i in range(3988):\n",
    "    labels.append(df['Label'][i])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from nltk import tokenize\n",
    "paras = []\n",
    "texts = []\n",
    "sent_lens = []\n",
    "sent_nums = []\n",
    "for idx in range(df.Body.shape[0]):\n",
    "    text = df.Body[idx]\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    sent_nums.append(len(sentences))\n",
    "    for sent in sentences:\n",
    "        sent_lens.append(len(text_to_word_sequence(sent)))\n",
    "    paras.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "maxlen_sentence = 100\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_seq_length = 1000\n",
    "# the percentage of train test split to be applied\n",
    "validation_split = 0.2\n",
    "# the dimension of vectors to be used\n",
    "embedding_dim = 100\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "max_features = 200000\n",
    "max_senten_len=100\n",
    "max_senten_num=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('./glove.6B.100d.txt', encoding='utf8')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df['Body'])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>four way bob corker skewer donald trump</td>\n",
       "      <td>imag copyright getti imag sunday morn donald t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>linklat s war veteran comedi speak modern amer...</td>\n",
       "      <td>london reuter last flag fli comedi drama vietn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>trump s fight corker jeopard legisl agenda</td>\n",
       "      <td>feud broke public view last week mr corker sai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>egypt s cheiron win tie up pemex mexican onsho...</td>\n",
       "      <td>mexico citi reuter egypt s cheiron hold limit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>jason aldean open snl vega tribut</td>\n",
       "      <td>countri singer jason aldean perform las vega s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0            four way bob corker skewer donald trump   \n",
       "1  linklat s war veteran comedi speak modern amer...   \n",
       "2         trump s fight corker jeopard legisl agenda   \n",
       "3  egypt s cheiron win tie up pemex mexican onsho...   \n",
       "4                  jason aldean open snl vega tribut   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  imag copyright getti imag sunday morn donald t...      1  \n",
       "1  london reuter last flag fli comedi drama vietn...      1  \n",
       "2  feud broke public view last week mr corker sai...      1  \n",
       "3  mexico citi reuter egypt s cheiron hold limit ...      1  \n",
       "4  countri singer jason aldean perform las vega s...      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imag copyright getti imag sunday morn donald t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>london reuter last flag fli comedi drama vietn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feud broke public view last week mr corker sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mexico citi reuter egypt s cheiron hold limit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>countri singer jason aldean perform las vega s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body\n",
       "0  imag copyright getti imag sunday morn donald t...\n",
       "1  london reuter last flag fli comedi drama vietn...\n",
       "2  feud broke public view last week mr corker sai...\n",
       "3  mexico citi reuter egypt s cheiron hold limit ...\n",
       "4  countri singer jason aldean perform las vega s..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_df = df['Label'].values\n",
    "x_df = df.drop(columns=['Label', 'Headline', 'URLs'], axis=1)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3190, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "798"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3190"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.Body.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for text in x_train.Body:\n",
    "    X_train.append(text)\n",
    "\n",
    "X_test = []\n",
    "for text in x_test.Body:\n",
    "    X_test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = sequence.pad_sequences(token.texts_to_sequences(X_train),  maxlen=2500)\n",
    "\n",
    "X_test = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3190"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3190, 100, 25)\n",
      "x_test shape: (798, 100, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((len(X_train), maxlen_sentence, maxlen_word))\n",
    "X_test = X_test.reshape((len(X_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "100/100 [==============================] - 349s 3s/step - loss: 0.3846 - accuracy: 0.8047 - val_loss: 0.1860 - val_accuracy: 0.9273\n",
      "Test...\n",
      "25/25 [==============================] - 26s 909ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "epochs = 1\n",
    "print('Build model...')\n",
    "model = HCAN(maxlen_sentence, maxlen_word, max_features, embedding_dims)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(X_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (result.reshape(1,-1)[0]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC 0.9273182957393483\n",
      "Prec 0.8753246753246753\n",
      "REC 0.9711815561959655\n",
      "F1 0.9207650273224044\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
    "print(\"Prec\", metrics.precision_score(result, y_test))\n",
    "print(\"REC\", metrics.recall_score(result, y_test))\n",
    "print(\"F1\", metrics.f1_score(result, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
